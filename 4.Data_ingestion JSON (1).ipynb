{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1514554-ddcc-4ee9-aaa0-b9ffda5c0356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/phule.rs@outlook.com/images/driver.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aab0d29a-4346-497c-8be9-917af45ada71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817a5f13-64e6-43d7-b101-e6df0f1f73af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67561f8b-04f6-4474-ab01-dd6be00e0393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name_schema = StructType(fields=[StructField(\"forename\", StringType(), True),\n",
    "                                 StructField(\"surname\", StringType(), True)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff8dd26-a353-4b33-b50b-33db7bb9ea64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_schema = StructType(fields=[StructField(\"driverId\", IntegerType(), False),\n",
    "                                    StructField(\"driverRef\", StringType(), True),\n",
    "                                    StructField(\"number\", IntegerType(), True),\n",
    "                                    StructField(\"code\", StringType(), True),\n",
    "                                    StructField(\"name\", name_schema),\n",
    "                                    StructField(\"dob\", DateType(), True),\n",
    "                                    StructField(\"nationality\", StringType(), True),\n",
    "                                    StructField(\"url\", StringType(), True)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f035f4-9b3f-4718-805d-095f0a36aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_df = spark.read \\\n",
    ".schema(drivers_schema) \\\n",
    ".json(\"dbfs:/FileStore/tables/raw/drivers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e508aa4b-6413-4202-9f2e-cff50a01a28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 2 - Rename columns and add new columns\n",
    "1. driverId renamed to driver_id  \n",
    "1. driverRef renamed to driver_ref  \n",
    "1. ingestion date added\n",
    "1. name added with concatenation of forename and surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edec4daf-ee43-494a-b97d-f71cfb8a853b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, current_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1353ae29-0f65-4b49-84a0-432c88b2e603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_with_columns_df = drivers_df.withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    "                                    .withColumnRenamed(\"driverRef\", \"driver_ref\") \\\n",
    "                                    .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "                                    .withColumn(\"name\", concat(col(\"name.forename\"), lit(\" \"), col(\"name.surname\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f950d07-a2f6-4742-955f-870404f7a5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 3 - Drop the unwanted columns\n",
    "1. name.forename\n",
    "1. name.surname\n",
    "1. url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679b5fed-b868-43e5-9e60-a3ef97be8c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_final_df = drivers_with_columns_df.drop(col(\"url\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "affe6707-ae19-4ebe-b51a-a0f78acdf1b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Step 4 - Write to output to processed container in parquet format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ce3622-0d47-4b82-8735-94a866a29ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drivers_final_df.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/tables/clean_data/drivers\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.Data_ingestion JSON (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}